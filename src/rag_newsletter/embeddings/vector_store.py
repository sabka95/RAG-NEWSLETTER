from typing import List, Dict, Any, Optional, Tuple
from langchain_community.vectorstores import Qdrant
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.http import models
import logging
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from loguru import logger

class OptimizedVectorStoreService:
    def __init__(self, 
                 qdrant_url: str = "http://localhost:6333",
                 collection_name: str = "rag_newsletter",
                 embedding_service=None,
                 use_binary_quantization: bool = True,
                 hnsw_config: Optional[Dict] = None):
        """
        Service de gestion du vector store Qdrant optimis√© pour Apple Silicon
        
        Args:
            qdrant_url: URL du serveur Qdrant
            collection_name: Nom de la collection
            embedding_service: Service d'embeddings MLX
            use_binary_quantization: Utiliser la quantization binaire pour √©conomiser l'espace
            hnsw_config: Configuration HNSW personnalis√©e
        """
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.embedding_service = embedding_service
        self.use_binary_quantization = use_binary_quantization
        self.client = None
        self.vector_store = None
        
        # Configuration HNSW optimis√©e pour Apple Silicon
        self.hnsw_config = hnsw_config or {
            "m": 16,  # Nombre de connexions pour chaque n≈ìud
            "ef_construct": 100,  # Taille de la liste dynamique pendant la construction
            "ef": 64,  # Taille de la liste dynamique pendant la recherche
            "full_scan_threshold": 10000,  # Seuil pour le scan complet
        }
        
        self._initialize_client()
    
    def _initialize_client(self):
        """Initialise le client Qdrant avec optimisations"""
        try:
            logger.info(f"üîó Connexion √† Qdrant: {self.qdrant_url}")
            self.client = QdrantClient(url=self.qdrant_url)
            
            # Cr√©er la collection si elle n'existe pas
            self._create_collection_if_not_exists()
            
            logger.info("‚úÖ Client Qdrant initialis√© avec optimisations HNSW")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation de Qdrant: {e}")
            raise
    
    def _create_collection_if_not_exists(self):
        """Cr√©e la collection avec optimisations HNSW et binary quantization"""
        try:
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.collection_name not in collection_names:
                logger.info(f"üèóÔ∏è  Cr√©ation de la collection optimis√©e: {self.collection_name}")
                
                # Configuration des vecteurs
                vector_config = models.VectorParams(
                    size=1536,  # Taille des embeddings MCDSE
                    distance=models.Distance.COSINE,
                    on_disk=True,  # Stockage sur disque pour √©conomiser la RAM
                )
                
                # Configuration HNSW
                hnsw_config = models.HnswConfigDiff(
                    m=self.hnsw_config["m"],
                    ef_construct=self.hnsw_config["ef_construct"],
                    full_scan_threshold=self.hnsw_config["full_scan_threshold"],
                    max_indexing_threads=0,  # Auto-d√©tection du nombre de threads
                )
                
                # Configuration de la quantization binaire si activ√©e
                quantization_config = None
                if self.use_binary_quantization:
                    quantization_config = models.BinaryQuantization(
                        binary=models.BinaryQuantizationConfig(
                            always_ram=True,  # Garder en RAM pour de meilleures performances
                        )
                    )
                
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=vector_config,
                    hnsw_config=hnsw_config,
                    quantization_config=quantization_config,
                )
                
                logger.info(f"‚úÖ Collection '{self.collection_name}' cr√©√©e avec HNSW + Binary Quantization")
            else:
                logger.info(f"‚ÑπÔ∏è  Collection '{self.collection_name}' existe d√©j√†")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation de la collection: {e}")
            raise
    
    def add_documents(self, documents: List[Document]) -> List[str]:
        """
        Ajoute des documents au vector store avec optimisations
        
        Args:
            documents: Liste de documents LangChain
            
        Returns:
            Liste des IDs des documents ajout√©s
        """
        if not self.client:
            raise RuntimeError("Client Qdrant non initialis√©")
        
        try:
            logger.info(f"üìö Ajout de {len(documents)} documents au vector store optimis√©")
            
            # G√©n√©rer les embeddings avec le mod√®le MCDSE
            logger.info("üñºÔ∏è  G√©n√©ration des embeddings MCDSE...")
            embeddings = self.embedding_service.embed_documents(documents)
            
            # Nettoyer les m√©tadonn√©es pour Qdrant
            cleaned_metadata_list = []
            for doc in documents:
                cleaned_metadata = {}
                for key, value in doc.metadata.items():
                    # Exclure les donn√©es binaires mais garder les autres m√©tadonn√©es
                    if key not in ['image_data', 'image_format'] and isinstance(value, (str, int, float, bool, list, dict)):
                        cleaned_metadata[key] = value
                cleaned_metadata_list.append(cleaned_metadata)
            
            # Ajouter les embeddings avec optimisations
            points = []
            for i, (doc, embedding, metadata) in enumerate(zip(documents, embeddings, cleaned_metadata_list)):
                points.append({
                    "id": i + 1,  # IDs commencent √† 1 (Qdrant n'accepte pas 0)
                    "vector": embedding,
                    "payload": {
                        "page_content": doc.page_content,
                        **metadata
                    }
                })
            
            # V√©rifier qu'il y a des points √† ins√©rer
            if not points:
                logger.warning("Aucun point √† ins√©rer dans Qdrant")
                return []
            
            # Ins√©rer dans Qdrant avec configuration optimis√©e
            self.client.upsert(
                collection_name=self.collection_name,
                points=points,
                wait=True,  # Attendre la confirmation
            )
            
            ids = [str(i + 1) for i in range(len(documents))]
            logger.info(f"‚úÖ Documents ajout√©s avec succ√®s: {len(ids)} embeddings optimis√©s")
            return ids
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ajout des documents: {e}")
            raise
    
    def similarity_search(self, 
                         query: str, 
                         k: int = 5,
                         filter: Optional[Dict] = None) -> List[Document]:
        """
        Recherche de similarit√© optimis√©e avec HNSW
        
        Args:
            query: Requ√™te de recherche
            k: Nombre de r√©sultats √† retourner
            filter: Filtres √† appliquer
            
        Returns:
            Liste des documents les plus similaires
        """
        if not self.client:
            raise RuntimeError("Client Qdrant non initialis√©")
        
        try:
            # G√©n√©rer l'embedding de la requ√™te
            query_embedding = self.embedding_service.embed_query(query)
            
            # Recherche optimis√©e avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config["ef"],  # Utiliser la configuration HNSW
                exact=False,  # Utiliser HNSW au lieu du scan exact
            )
            
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )
            
            # Convertir les r√©sultats en documents LangChain
            documents = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={k: v for k, v in result.payload.items() if k != "page_content"}
                )
                documents.append(doc)
            
            logger.info(f"üîç Recherche HNSW termin√©e: {len(documents)} r√©sultats")
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            raise
    
    def similarity_search_with_score(self, 
                                   query: str, 
                                   k: int = 5,
                                   filter: Optional[Dict] = None) -> List[Tuple[Document, float]]:
        """
        Recherche de similarit√© avec scores et optimisations HNSW
        
        Args:
            query: Requ√™te de recherche
            k: Nombre de r√©sultats √† retourner
            filter: Filtres √† appliquer
            
        Returns:
            Liste de tuples (document, score)
        """
        if not self.client:
            raise RuntimeError("Client Qdrant non initialis√©")
        
        try:
            # G√©n√©rer l'embedding de la requ√™te
            query_embedding = self.embedding_service.embed_query(query)
            
            # Recherche optimis√©e avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config["ef"],
                exact=False,
            )
            
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )
            
            # Convertir les r√©sultats en documents LangChain avec scores
            documents_with_scores = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={k: v for k, v in result.payload.items() if k != "page_content"}
                )
                documents_with_scores.append((doc, result.score))
            
            logger.info(f"üîç Recherche HNSW avec scores termin√©e: {len(documents_with_scores)} r√©sultats")
            return documents_with_scores
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche avec scores: {e}")
            raise
    
    def mmr_search(self, 
                   query: str, 
                   k: int = 5,
                   lambda_mult: float = 0.7,
                   filter: Optional[Dict] = None) -> List[Tuple[Document, float]]:
        """
        Recherche Maximum Marginal Relevance (MMR) pour diversifier les r√©sultats
        
        Args:
            query: Requ√™te de recherche
            k: Nombre de r√©sultats √† retourner
            lambda_mult: Facteur de diversit√© (0.0 = max diversit√©, 1.0 = max pertinence)
            filter: Filtres √† appliquer
            
        Returns:
            Liste de tuples (document, score_mmr)
        """
        if not self.client:
            raise RuntimeError("Client Qdrant non initialis√©")
        
        try:
            # R√©cup√©rer plus de r√©sultats pour l'algorithme MMR
            fetch_k = min(k * 3, 50)  # R√©cup√©rer 3x plus de r√©sultats
            
            # Recherche initiale avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config["ef"],
                exact=False,
            )
            
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=self.embedding_service.embed_query(query),
                limit=fetch_k,
                with_payload=True,
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )
            
            if not results:
                return []
            
            # Convertir en documents avec scores
            candidate_docs = []
            candidate_scores = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={k: v for k, v in result.payload.items() if k != "page_content"}
                )
                candidate_docs.append(doc)
                candidate_scores.append(result.score)
            
            # Appliquer l'algorithme MMR
            selected_docs = self._apply_mmr(
                query_embedding=self.embedding_service.embed_query(query),
                candidate_docs=candidate_docs,
                candidate_scores=candidate_scores,
                k=k,
                lambda_mult=lambda_mult
            )
            
            logger.info(f"üéØ Recherche MMR termin√©e: {len(selected_docs)} r√©sultats diversifi√©s")
            return selected_docs
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche MMR: {e}")
            raise
    
    def _apply_mmr(self, 
                   query_embedding: List[float], 
                   candidate_docs: List[Document], 
                   candidate_scores: List[float],
                   k: int, 
                   lambda_mult: float) -> List[Tuple[Document, float]]:
        """
        Applique l'algorithme Maximum Marginal Relevance
        
        Args:
            query_embedding: Embedding de la requ√™te
            candidate_docs: Documents candidats
            candidate_scores: Scores de similarit√© initiaux
            k: Nombre de r√©sultats √† s√©lectionner
            lambda_mult: Facteur de diversit√©
            
        Returns:
            Liste des documents s√©lectionn√©s avec scores MMR
        """
        if len(candidate_docs) <= k:
            return list(zip(candidate_docs, candidate_scores))
        
        # Convertir en numpy pour les calculs
        query_vec = np.array(query_embedding).reshape(1, -1)
        
        # Calculer les embeddings des documents candidats
        doc_embeddings = []
        for doc in candidate_docs:
            # Re-g√©n√©rer l'embedding du document (ou utiliser le cache si disponible)
            doc_embedding = self._get_document_embedding(doc)
            # S'assurer que l'embedding est un array 1D
            if isinstance(doc_embedding, list):
                doc_embedding = np.array(doc_embedding)
            doc_embeddings.append(doc_embedding.flatten())
        
        doc_embeddings = np.array(doc_embeddings)
        
        # Algorithme MMR
        selected_indices = []
        remaining_indices = list(range(len(candidate_docs)))
        
        # S√©lectionner le premier document (le plus pertinent)
        best_idx = np.argmax(candidate_scores)
        selected_indices.append(best_idx)
        remaining_indices.remove(best_idx)
        
        # S√©lectionner les documents restants
        for _ in range(min(k - 1, len(remaining_indices))):
            mmr_scores = []
            
            for idx in remaining_indices:
                # Score de pertinence (similarit√© avec la requ√™te)
                relevance = cosine_similarity(
                    query_vec, 
                    doc_embeddings[idx].reshape(1, -1)
                )[0][0]
                
                # Score de diversit√© (similarit√© maximale avec les documents d√©j√† s√©lectionn√©s)
                if selected_indices:
                    max_similarity = max([
                        cosine_similarity(
                            doc_embeddings[idx].reshape(1, -1),
                            doc_embeddings[sel_idx].reshape(1, -1)
                        )[0][0]
                        for sel_idx in selected_indices
                    ])
                else:
                    max_similarity = 0
                
                # Score MMR
                mmr_score = lambda_mult * relevance - (1 - lambda_mult) * max_similarity
                mmr_scores.append(mmr_score)
            
            # S√©lectionner le document avec le meilleur score MMR
            best_idx = remaining_indices[np.argmax(mmr_scores)]
            selected_indices.append(best_idx)
            remaining_indices.remove(best_idx)
        
        # Retourner les documents s√©lectionn√©s avec leurs scores initiaux
        selected_docs = []
        for idx in selected_indices:
            selected_docs.append((candidate_docs[idx], candidate_scores[idx]))
        
        return selected_docs
    
    def _get_document_embedding(self, doc: Document) -> List[float]:
        """
        R√©cup√®re l'embedding d'un document (utilise le cache si disponible)
        
        Args:
            doc: Document LangChain
            
        Returns:
            Embedding du document
        """
        # Si l'embedding est d√©j√† en cache dans les m√©tadonn√©es
        if 'cached_embedding' in doc.metadata:
            return doc.metadata['cached_embedding']
        
        # Sinon, re-g√©n√©rer l'embedding
        # Note: Dans un syst√®me de production, vous voudriez utiliser un cache Redis
        return self.embedding_service.embed_documents([doc])[0]
    
    def _build_filter(self, filter_dict: Dict) -> models.Filter:
        """
        Construit un filtre Qdrant √† partir d'un dictionnaire
        
        Args:
            filter_dict: Dictionnaire de filtres
            
        Returns:
            Filtre Qdrant
        """
        conditions = []
        
        for key, value in filter_dict.items():
            if isinstance(value, list):
                conditions.append(
                    models.FieldCondition(
                        key=key,
                        match=models.MatchAny(any=value)
                    )
                )
            else:
                conditions.append(
                    models.FieldCondition(
                        key=key,
                        match=models.MatchValue(value=value)
                    )
                )
        
        return models.Filter(must=conditions)
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        Retourne les informations sur la collection
        
        Returns:
            Dictionnaire avec les informations de la collection
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return {
                "name": collection_info.config.params.vectors.size,
                "vectors_count": collection_info.vectors_count,
                "indexed_vectors_count": collection_info.indexed_vectors_count,
                "points_count": collection_info.points_count,
                "segments_count": collection_info.segments_count,
                "status": collection_info.status,
                "optimizer_status": collection_info.optimizer_status,
            }
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des infos de collection: {e}")
            return {}

# Alias pour la compatibilit√©
VectorStoreService = OptimizedVectorStoreService