# =============================================================================
# RAG Newsletter - Service de Vector Store Optimis√©
# =============================================================================
# Service de gestion du vector store Qdrant optimis√© pour Apple Silicon avec
# HNSW indexing, Binary Quantization et MMR search pour des performances maximales.
# =============================================================================

from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from langchain.schema import Document
from loguru import logger
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sklearn.metrics.pairwise import cosine_similarity


class OptimizedVectorStoreService:
    """
    Service de gestion du vector store Qdrant optimis√© pour Apple Silicon M4.

    Ce service utilise Qdrant comme base de donn√©es vectorielle avec des optimisations
    avanc√©es pour les processeurs Apple Silicon :
    - HNSW indexing pour des recherches ultra-rapides
    - Binary Quantization pour √©conomiser 75% d'espace de stockage
    - MMR (Maximum Marginal Relevance) pour diversifier les r√©sultats
    - Configuration optimis√©e pour les processeurs M4

    Fonctionnalit√©s cl√©s:
    - Stockage et recherche d'embeddings vectoriels
    - Recherche de similarit√© avec scores
    - Recherche MMR pour diversifier les r√©sultats
    - Filtrage par m√©tadonn√©es
    - Gestion des collections optimis√©es

    Exemple d'utilisation:
        >>> service = OptimizedVectorStoreService(embedding_service=mlx_service)
        >>> service.add_documents(documents)
        >>> results = service.similarity_search("sustainability strategy", k=5)
    """

    def __init__(
        self,
        qdrant_url: str = "http://localhost:6333",
        collection_name: str = "rag_newsletter",
        embedding_service=None,
        use_binary_quantization: bool = True,
        hnsw_config: Optional[Dict] = None,
    ):
        """
        Initialise le service de vector store optimis√©.

        Args:
            qdrant_url (str): URL du serveur Qdrant (d√©faut: "http://localhost:6333")
            collection_name (str): Nom de la collection Qdrant (d√©faut: "rag_newsletter")
            embedding_service: Service d'embeddings MLX pour g√©n√©rer les vecteurs
            use_binary_quantization (bool): Activer la quantization binaire pour √©conomiser l'espace
            hnsw_config (Optional[Dict]): Configuration HNSW personnalis√©e

        Raises:
            RuntimeError: Si la connexion √† Qdrant √©choue
        """
        self.qdrant_url = qdrant_url
        self.collection_name = collection_name
        self.embedding_service = embedding_service
        self.use_binary_quantization = use_binary_quantization
        self.client = None
        self.vector_store = None

        # Configuration HNSW optimis√©e pour Apple Silicon M4
        # HNSW (Hierarchical Navigable Small World) est un algorithme de recherche
        # vectorielle qui permet des recherches ultra-rapides m√™me sur de gros volumes
        self.hnsw_config = hnsw_config or {
            "m": 16,  # Nombre de connexions pour chaque n≈ìud (optimis√© pour M4)
            "ef_construct": 100,  # Taille de la liste dynamique pendant la construction
            "ef": 64,  # Taille de la liste dynamique pendant la recherche
            "full_scan_threshold": 10000,  # Seuil pour le scan complet (performance)
        }

        # Initialiser la connexion √† Qdrant
        self._initialize_client()

    def _initialize_client(self):
        """
        Initialise le client Qdrant avec optimisations pour Apple Silicon.

        Cette m√©thode √©tablit la connexion √† Qdrant et configure la collection
        avec les param√®tres optimis√©s pour les processeurs Apple Silicon M4.

        Raises:
            RuntimeError: Si la connexion √† Qdrant √©choue
        """
        try:
            logger.info(f"üîó Connexion √† Qdrant: {self.qdrant_url}")
            self.client = QdrantClient(url=self.qdrant_url)

            # Cr√©er la collection si elle n'existe pas avec les optimisations
            self._create_collection_if_not_exists()

            logger.info("‚úÖ Client Qdrant initialis√© avec optimisations HNSW")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation de Qdrant: {e}")
            raise RuntimeError(f"Impossible de se connecter √† Qdrant: {e}")

    def _create_collection_if_not_exists(self):
        """
        Cr√©e la collection Qdrant avec optimisations HNSW et Binary Quantization.

        Cette m√©thode cr√©e une collection Qdrant optimis√©e pour Apple Silicon M4
        avec les param√®tres suivants :
        - HNSW indexing pour des recherches ultra-rapides
        - Binary Quantization pour √©conomiser 75% d'espace de stockage
        - Configuration optimis√©e pour les processeurs M4
        - Stockage sur disque pour √©conomiser la RAM

        Raises:
            RuntimeError: Si la cr√©ation de la collection √©choue
        """
        try:
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]

            if self.collection_name not in collection_names:
                logger.info(
                    f"üèóÔ∏è  Cr√©ation de la collection optimis√©e: {self.collection_name}"
                )

                # Configuration des vecteurs pour MCDSE-2B-V1
                vector_config = models.VectorParams(
                    size=1536,  # Taille des embeddings MCDSE-2B-V1 (1536 dimensions)
                    distance=models.Distance.COSINE,  # Distance cosinus pour la similarit√©
                    on_disk=True,  # Stockage sur disque pour √©conomiser la RAM
                )

                # Configuration HNSW optimis√©e pour Apple Silicon M4
                hnsw_config = models.HnswConfigDiff(
                    m=self.hnsw_config["m"],  # Connexions par n≈ìud (optimis√© pour M4)
                    ef_construct=self.hnsw_config[
                        "ef_construct"
                    ],  # Construction de l'index
                    full_scan_threshold=self.hnsw_config[
                        "full_scan_threshold"
                    ],  # Seuil de scan
                    max_indexing_threads=0,  # Auto-d√©tection du nombre de threads (utilise tous les cores M4)
                )

                # Configuration de la quantization binaire si activ√©e
                quantization_config = None
                if self.use_binary_quantization:
                    quantization_config = models.BinaryQuantization(
                        binary=models.BinaryQuantizationConfig(
                            always_ram=True,  # Garder en RAM pour de meilleures performances
                        )
                    )

                # Cr√©er la collection avec toutes les optimisations
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=vector_config,
                    hnsw_config=hnsw_config,
                    quantization_config=quantization_config,
                )

                logger.info(
                    f"‚úÖ Collection '{self.collection_name}' cr√©√©e avec HNSW + Binary Quantization"
                )
            else:
                logger.info(f"‚ÑπÔ∏è  Collection '{self.collection_name}' existe d√©j√†")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la cr√©ation de la collection: {e}")
            raise RuntimeError(f"Impossible de cr√©er la collection Qdrant: {e}")

    def add_documents(self, documents: List[Document]) -> List[str]:
        """
        Ajoute des documents au vector store avec optimisations pour Apple Silicon.

        Cette m√©thode traite une liste de documents LangChain, g√©n√®re leurs embeddings
        avec le service MLX, et les stocke dans Qdrant avec les optimisations HNSW
        et Binary Quantization.

        Args:
            documents (List[Document]): Liste de documents LangChain √† ajouter.
                                      Chaque document doit avoir 'image_data' dans ses m√©tadonn√©es.

        Returns:
            List[str]: Liste des IDs des documents ajout√©s dans Qdrant

        Raises:
            RuntimeError: Si le client Qdrant n'est pas initialis√© ou si l'ajout √©choue

        Exemple:
            >>> service = OptimizedVectorStoreService(embedding_service=mlx_service)
            >>> documents = [doc1, doc2, doc3]  # Documents avec image_data
            >>> ids = service.add_documents(documents)
            >>> print(f"Ajout√© {len(ids)} documents")
        """
        if not self.client:
            raise RuntimeError(
                "Client Qdrant non initialis√©. Appelez d'abord __init__()"
            )

        try:
            logger.info(
                f"üìö Ajout de {len(documents)} documents au vector store optimis√©"
            )

            # G√©n√©rer les embeddings avec le mod√®le MCDSE-2B-V1
            logger.info("üñºÔ∏è  G√©n√©ration des embeddings MCDSE...")
            embeddings = self.embedding_service.embed_documents(documents)

            # Nettoyer les m√©tadonn√©es pour Qdrant
            # Qdrant ne peut pas stocker de donn√©es binaires dans les m√©tadonn√©es
            cleaned_metadata_list = []
            for doc in documents:
                cleaned_metadata = {}
                for key, value in doc.metadata.items():
                    # Exclure les donn√©es binaires mais garder les autres m√©tadonn√©es
                    if key not in ["image_data", "image_format"] and isinstance(
                        value, (str, int, float, bool, list, dict)
                    ):
                        cleaned_metadata[key] = value
                cleaned_metadata_list.append(cleaned_metadata)

            # Pr√©parer les points pour l'insertion dans Qdrant
            points = []
            for i, (doc, embedding, metadata) in enumerate(
                zip(documents, embeddings, cleaned_metadata_list)
            ):
                points.append(
                    {
                        "id": i + 1,  # IDs commencent √† 1 (Qdrant n'accepte pas 0)
                        "vector": embedding,  # Vecteur d'embedding de 1536 dimensions
                        "payload": {
                            "page_content": doc.page_content,  # Contenu textuel du document
                            **metadata,  # M√©tadonn√©es nettoy√©es
                        },
                    }
                )

            # V√©rifier qu'il y a des points √† ins√©rer
            if not points:
                logger.warning("Aucun point √† ins√©rer dans Qdrant")
                return []

            # Ins√©rer dans Qdrant avec configuration optimis√©e
            self.client.upsert(
                collection_name=self.collection_name,
                points=points,
                wait=True,  # Attendre la confirmation de l'insertion
            )

            ids = [str(i + 1) for i in range(len(documents))]
            logger.info(
                f"‚úÖ Documents ajout√©s avec succ√®s: {len(ids)} embeddings optimis√©s"
            )
            return ids

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ajout des documents: {e}")
            raise RuntimeError(f"Impossible d'ajouter les documents: {e}")

    def similarity_search(
        self, query: str, k: int = 5, filter: Optional[Dict] = None
    ) -> List[Document]:
        """
        Recherche de similarit√© optimis√©e avec HNSW pour Apple Silicon.

        Cette m√©thode effectue une recherche vectorielle rapide dans la collection
        Qdrant en utilisant l'algorithme HNSW optimis√© pour les processeurs M4.

        Args:
            query (str): Requ√™te textuelle √† rechercher (ex: "sustainability strategy")
            k (int): Nombre de r√©sultats √† retourner (d√©faut: 5)
            filter (Optional[Dict]): Filtres de m√©tadonn√©es √† appliquer

        Returns:
            List[Document]: Liste des documents les plus similaires √† la requ√™te

        Raises:
            RuntimeError: Si le client Qdrant n'est pas initialis√© ou si la recherche √©choue

        Exemple:
            >>> service = OptimizedVectorStoreService(embedding_service=mlx_service)
            >>> results = service.similarity_search("climate change", k=3)
            >>> print(f"Trouv√© {len(results)} documents similaires")
        """
        if not self.client:
            raise RuntimeError(
                "Client Qdrant non initialis√©. Appelez d'abord __init__()"
            )

        try:
            # G√©n√©rer l'embedding de la requ√™te avec le service MLX
            query_embedding = self.embedding_service.embed_query(query)

            # Recherche optimis√©e avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config[
                    "ef"
                ],  # Utiliser la configuration HNSW optimis√©e
                exact=False,  # Utiliser HNSW au lieu du scan exact (plus rapide)
            )

            # Effectuer la recherche dans Qdrant
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,  # Inclure les m√©tadonn√©es
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )

            # Convertir les r√©sultats en documents LangChain
            documents = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={
                        k: v for k, v in result.payload.items() if k != "page_content"
                    },
                )
                documents.append(doc)

            logger.info(f"üîç Recherche HNSW termin√©e: {len(documents)} r√©sultats")
            return documents

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            raise RuntimeError(f"Impossible d'effectuer la recherche: {e}")

    def similarity_search_with_score(
        self, query: str, k: int = 5, filter: Optional[Dict] = None
    ) -> List[Tuple[Document, float]]:
        """
        Recherche de similarit√© avec scores et optimisations HNSW pour Apple Silicon.

        Cette m√©thode effectue une recherche vectorielle rapide et retourne les documents
        avec leurs scores de similarit√©, permettant d'√©valuer la pertinence des r√©sultats.

        Args:
            query (str): Requ√™te textuelle √† rechercher (ex: "sustainability strategy")
            k (int): Nombre de r√©sultats √† retourner (d√©faut: 5)
            filter (Optional[Dict]): Filtres de m√©tadonn√©es √† appliquer

        Returns:
            List[Tuple[Document, float]]: Liste de tuples (document, score_de_similarit√©)
                                        Les scores sont entre 0 et 1 (1 = parfaitement similaire)

        Raises:
            RuntimeError: Si le client Qdrant n'est pas initialis√© ou si la recherche √©choue

        Exemple:
            >>> service = OptimizedVectorStoreService(embedding_service=mlx_service)
            >>> results = service.similarity_search_with_score("climate change", k=3)
            >>> for doc, score in results:
            ...     print(f"Score: {score:.3f} - {doc.page_content[:50]}...")
        """
        if not self.client:
            raise RuntimeError(
                "Client Qdrant non initialis√©. Appelez d'abord __init__()"
            )

        try:
            # G√©n√©rer l'embedding de la requ√™te avec le service MLX
            query_embedding = self.embedding_service.embed_query(query)

            # Recherche optimis√©e avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config[
                    "ef"
                ],  # Utiliser la configuration HNSW optimis√©e
                exact=False,  # Utiliser HNSW au lieu du scan exact (plus rapide)
            )

            # Effectuer la recherche dans Qdrant
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=k,
                with_payload=True,  # Inclure les m√©tadonn√©es
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )

            # Convertir les r√©sultats en documents LangChain avec scores
            documents_with_scores = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={
                        k: v for k, v in result.payload.items() if k != "page_content"
                    },
                )
                documents_with_scores.append((doc, result.score))

            logger.info(
                f"üîç Recherche HNSW avec scores termin√©e: {len(documents_with_scores)} r√©sultats"
            )
            return documents_with_scores

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche avec scores: {e}")
            raise RuntimeError(f"Impossible d'effectuer la recherche avec scores: {e}")

    def mmr_search(
        self,
        query: str,
        k: int = 5,
        lambda_mult: float = 0.7,
        filter: Optional[Dict] = None,
    ) -> List[Tuple[Document, float]]:
        """
        Recherche MMR (Maximum Marginal Relevance) pour diversifier les r√©sultats.

        Cette m√©thode utilise l'algorithme MMR pour s√©lectionner des documents
        qui sont √† la fois pertinents par rapport √† la requ√™te et diversifi√©s
        entre eux, √©vitant la redondance dans les r√©sultats.

        Args:
            query (str): Requ√™te textuelle √† rechercher (ex: "sustainability strategy")
            k (int): Nombre de r√©sultats √† retourner (d√©faut: 5)
            lambda_mult (float): Facteur de diversit√© (0.0 = max diversit√©, 1.0 = max pertinence)
            filter (Optional[Dict]): Filtres de m√©tadonn√©es √† appliquer

        Returns:
            List[Tuple[Document, float]]: Liste de tuples (document, score_mmr)
                                        Les scores MMR combinent pertinence et diversit√©

        Raises:
            RuntimeError: Si le client Qdrant n'est pas initialis√© ou si la recherche √©choue

        Exemple:
            >>> service = OptimizedVectorStoreService(embedding_service=mlx_service)
            >>> results = service.mmr_search("sustainability", k=3, lambda_mult=0.5)
            >>> for doc, score in results:
            ...     print(f"Score MMR: {score:.3f} - {doc.page_content[:50]}...")
        """
        if not self.client:
            raise RuntimeError(
                "Client Qdrant non initialis√©. Appelez d'abord __init__()"
            )

        try:
            # R√©cup√©rer plus de r√©sultats pour l'algorithme MMR
            # MMR a besoin de plus de candidats pour bien diversifier
            fetch_k = min(k * 3, 50)  # R√©cup√©rer 3x plus de r√©sultats (max 50)

            # Recherche initiale avec HNSW
            search_params = models.SearchParams(
                hnsw_ef=self.hnsw_config[
                    "ef"
                ],  # Utiliser la configuration HNSW optimis√©e
                exact=False,  # Utiliser HNSW au lieu du scan exact (plus rapide)
            )

            # Effectuer la recherche dans Qdrant
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=self.embedding_service.embed_query(query),
                limit=fetch_k,
                with_payload=True,  # Inclure les m√©tadonn√©es
                search_params=search_params,
                query_filter=self._build_filter(filter) if filter else None,
            )

            if not results:
                return []

            # Convertir en documents avec scores
            candidate_docs = []
            candidate_scores = []
            for result in results:
                doc = Document(
                    page_content=result.payload.get("page_content", ""),
                    metadata={
                        k: v for k, v in result.payload.items() if k != "page_content"
                    },
                )
                candidate_docs.append(doc)
                candidate_scores.append(result.score)

            # Appliquer l'algorithme MMR pour diversifier les r√©sultats
            selected_docs = self._apply_mmr(
                query_embedding=self.embedding_service.embed_query(query),
                candidate_docs=candidate_docs,
                candidate_scores=candidate_scores,
                k=k,
                lambda_mult=lambda_mult,
            )

            logger.info(
                f"üéØ Recherche MMR termin√©e: {len(selected_docs)} r√©sultats diversifi√©s"
            )
            return selected_docs

        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la recherche MMR: {e}")
            raise RuntimeError(f"Impossible d'effectuer la recherche MMR: {e}")

    def _apply_mmr(
        self,
        query_embedding: List[float],
        candidate_docs: List[Document],
        candidate_scores: List[float],
        k: int,
        lambda_mult: float,
    ) -> List[Tuple[Document, float]]:
        """
        Applique l'algorithme MMR (Maximum Marginal Relevance) pour diversifier les r√©sultats.

        L'algorithme MMR s√©lectionne des documents qui maximisent la pertinence
        par rapport √† la requ√™te tout en minimisant la redondance entre les r√©sultats.

        Args:
            query_embedding (List[float]): Embedding de la requ√™te (1536 dimensions)
            candidate_docs (List[Document]): Documents candidats √† diversifier
            candidate_scores (List[float]): Scores de similarit√© initiaux des candidats
            k (int): Nombre de r√©sultats √† s√©lectionner
            lambda_mult (float): Facteur de diversit√© (0.0 = max diversit√©, 1.0 = max pertinence)

        Returns:
            List[Tuple[Document, float]]: Liste des documents s√©lectionn√©s avec scores MMR
                                        Les scores MMR combinent pertinence et diversit√©
        """
        if len(candidate_docs) <= k:
            return list(zip(candidate_docs, candidate_scores))

        # Convertir en numpy pour les calculs
        query_vec = np.array(query_embedding).reshape(1, -1)

        # Calculer les embeddings des documents candidats
        doc_embeddings = []
        for doc in candidate_docs:
            # Re-g√©n√©rer l'embedding du document (ou utiliser le cache si disponible)
            doc_embedding = self._get_document_embedding(doc)
            # S'assurer que l'embedding est un array 1D
            if isinstance(doc_embedding, list):
                doc_embedding = np.array(doc_embedding)
            doc_embeddings.append(doc_embedding.flatten())

        doc_embeddings = np.array(doc_embeddings)

        # Algorithme MMR
        selected_indices = []
        remaining_indices = list(range(len(candidate_docs)))

        # S√©lectionner le premier document (le plus pertinent)
        best_idx = np.argmax(candidate_scores)
        selected_indices.append(best_idx)
        remaining_indices.remove(best_idx)

        # S√©lectionner les documents restants
        for _ in range(min(k - 1, len(remaining_indices))):
            mmr_scores = []

            for idx in remaining_indices:
                # Score de pertinence (similarit√© avec la requ√™te)
                relevance = cosine_similarity(
                    query_vec, doc_embeddings[idx].reshape(1, -1)
                )[0][0]

                # Score de diversit√© (similarit√© maximale avec les documents d√©j√† s√©lectionn√©s)
                if selected_indices:
                    max_similarity = max(
                        [
                            cosine_similarity(
                                doc_embeddings[idx].reshape(1, -1),
                                doc_embeddings[sel_idx].reshape(1, -1),
                            )[0][0]
                            for sel_idx in selected_indices
                        ]
                    )
                else:
                    max_similarity = 0

                # Score MMR
                mmr_score = lambda_mult * relevance - (1 - lambda_mult) * max_similarity
                mmr_scores.append(mmr_score)

            # S√©lectionner le document avec le meilleur score MMR
            best_idx = remaining_indices[np.argmax(mmr_scores)]
            selected_indices.append(best_idx)
            remaining_indices.remove(best_idx)

        # Retourner les documents s√©lectionn√©s avec leurs scores initiaux
        selected_docs = []
        for idx in selected_indices:
            selected_docs.append((candidate_docs[idx], candidate_scores[idx]))

        return selected_docs

    def _get_document_embedding(self, doc: Document) -> List[float]:
        """
        R√©cup√®re l'embedding d'un document en utilisant le cache si disponible.

        Cette m√©thode optimise les performances en √©vitant de re-g√©n√©rer
        les embeddings d√©j√† calcul√©s, ce qui est crucial pour l'algorithme MMR.

        Args:
            doc (Document): Document LangChain dont on veut l'embedding

        Returns:
            List[float]: Embedding du document (1536 dimensions)

        Note:
            Dans un syst√®me de production, un cache Redis serait plus appropri√©
            pour g√©rer les embeddings de mani√®re distribu√©e.
        """
        # Si l'embedding est d√©j√† en cache dans les m√©tadonn√©es
        if "cached_embedding" in doc.metadata:
            return doc.metadata["cached_embedding"]

        # Sinon, re-g√©n√©rer l'embedding avec le service MLX
        # Note: Dans un syst√®me de production, vous voudriez utiliser un cache Redis
        return self.embedding_service.embed_documents([doc])[0]

    def _build_filter(self, filter_dict: Dict) -> models.Filter:
        """
        Construit un filtre Qdrant √† partir d'un dictionnaire de conditions.

        Cette m√©thode convertit un dictionnaire Python en filtre Qdrant,
        permettant de filtrer les r√©sultats de recherche par m√©tadonn√©es.

        Args:
            filter_dict (Dict): Dictionnaire de filtres avec les cl√©s suivantes :
                - str/int/float: Valeur exacte √† matcher
                - list: Valeurs √† matcher (OR)

        Returns:
            models.Filter: Filtre Qdrant configur√©

        Exemple:
            >>> filter_dict = {
            ...     "source_file": "document.pdf",
            ...     "category": ["sustainability", "climate"]
            ... }
            >>> filter = service._build_filter(filter_dict)
        """
        conditions = []

        for key, value in filter_dict.items():
            if isinstance(value, list):
                # Filtre par valeurs multiples (OR)
                conditions.append(
                    models.FieldCondition(key=key, match=models.MatchAny(any=value))
                )
            else:
                # Filtre par valeur exacte
                conditions.append(
                    models.FieldCondition(key=key, match=models.MatchValue(value=value))
                )

        return models.Filter(must=conditions)

    def get_collection_info(self) -> Dict[str, Any]:
        """
        Retourne les informations d√©taill√©es sur la collection Qdrant.

        Cette m√©thode fournit des m√©triques utiles pour surveiller l'√©tat
        et les performances de la collection vectorielle.

        Returns:
            Dict[str, Any]: Dictionnaire avec les informations de la collection :
                - name: Nom de la collection
                - vectors_count: Nombre total de vecteurs
                - indexed_vectors_count: Nombre de vecteurs index√©s
                - points_count: Nombre total de points
                - segments_count: Nombre de segments
                - status: Statut de la collection
                - optimizer_status: Statut de l'optimiseur

        Raises:
            RuntimeError: Si la r√©cup√©ration des informations √©choue
        """
        try:
            collection_info = self.client.get_collection(self.collection_name)
            return {
                "name": self.collection_name,
                "vectors_count": collection_info.vectors_count,
                "indexed_vectors_count": collection_info.indexed_vectors_count,
                "points_count": collection_info.points_count,
                "segments_count": collection_info.segments_count,
                "status": collection_info.status,
                "optimizer_status": collection_info.optimizer_status,
            }
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des infos de collection: {e}")
            return {}


# =============================================================================
# Alias de compatibilit√©
# =============================================================================
# Alias pour maintenir la compatibilit√© avec l'ancienne API
# Permet d'utiliser 'VectorStoreService' au lieu de 'OptimizedVectorStoreService'
# =============================================================================
VectorStoreService = OptimizedVectorStoreService
