from typing import List, Dict, Any, Optional
from langchain.schema import Document
import logging
import math
import numpy as np
from PIL import Image
import io
import mlx.core as mx
import mlx.nn as nn
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
import torch

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(self, 
                 model_name: str = "marco/mcdse-2b-v1",
                 dimension: int = 1024,
                 use_mlx: bool = True,
                 binary_quantization: bool = True):
        """
        Service d'embeddings utilisant le mod√®le MCDSE avec MLX pour Apple Silicon
        
        Args:
            model_name: Nom du mod√®le HuggingFace √† utiliser
            dimension: Dimension des embeddings de sortie
            use_mlx: Utiliser MLX pour l'optimisation Apple Silicon
            binary_quantization: Activer la binary quantization
        """
        self.model_name = model_name
        self.dimension = dimension
        self.use_mlx = use_mlx
        self.binary_quantization = binary_quantization
        
        # Configuration des prompts
        self.document_prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>What is shown in this image?<|im_end|>\n<|endoftext|>"
        self.query_prompt = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>Query: %s<|im_end|>\n<|endoftext|>"
        
        # Configuration des pixels
        self.min_pixels = 1 * 28 * 28
        self.max_pixels = 960 * 28 * 28
        
        self.model = None
        self.processor = None
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialise le mod√®le MCDSE avec MLX"""
        try:
            logger.info(f"üöÄ Chargement du mod√®le MCDSE: {self.model_name}")
            logger.info("üçé Optimisation Apple Silicon avec MLX")
            
            if self.use_mlx:
                logger.info("üì• Chargement avec MLX...")
                # Pour MLX, on utilise d'abord le mod√®le standard puis on le convertit
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.model_name,
                    attn_implementation="flash_attention_2",
                    torch_dtype=torch.bfloat16,
                    device_map="cpu"  # On charge sur CPU puis on optimise avec MLX
                ).eval()
            else:
                logger.info("üì• Chargement standard...")
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    self.model_name,
                    attn_implementation="flash_attention_2",
                    torch_dtype=torch.bfloat16,
                    device_map="auto"
                ).eval()
            
            logger.info("üì• Chargement du processeur...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                min_pixels=self.min_pixels,
                max_pixels=self.max_pixels
            )
            
            # Configuration du padding
            self.model.padding_side = "left"
            self.processor.tokenizer.padding_side = "left"
            
            logger.info("‚úÖ Mod√®le MCDSE charg√© avec succ√®s!")
            logger.info(f"üìä Dimension des embeddings: {self.dimension}")
            logger.info(f"üçé MLX activ√©: {self.use_mlx}")
            logger.info(f"üî¢ Binary quantization: {self.binary_quantization}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise
    
    def _round_by_factor(self, number: float, factor: int) -> int:
        """Arrondit un nombre par un facteur"""
        return round(number / factor) * factor
    
    def _ceil_by_factor(self, number: float, factor: int) -> int:
        """Arrondit vers le haut par un facteur"""
        return math.ceil(number / factor) * factor
    
    def _floor_by_factor(self, number: float, factor: int) -> int:
        """Arrondit vers le bas par un facteur"""
        return math.floor(number / factor) * factor
    
    def _smart_resize(self, height: int, width: int) -> tuple[int, int]:
        """Redimensionne intelligemment une image selon les contraintes du mod√®le"""
        h_bar = max(28, self._round_by_factor(height, 28))
        w_bar = max(28, self._round_by_factor(width, 28))
        
        if h_bar * w_bar > self.max_pixels:
            beta = math.sqrt((height * width) / self.max_pixels)
            h_bar = self._floor_by_factor(height / beta, 28)
            w_bar = self._floor_by_factor(width / beta, 28)
        elif h_bar * w_bar < self.min_pixels:
            beta = math.sqrt(self.min_pixels / (height * width))
            h_bar = self._ceil_by_factor(height * beta, 28)
            w_bar = self._ceil_by_factor(width * beta, 28)
        
        return h_bar, w_bar
    
    def _resize_image(self, image: Image.Image) -> Image.Image:
        """Redimensionne une image selon les contraintes du mod√®le"""
        new_size = self._smart_resize(image.height, image.width)
        return image.resize(new_size)
    
    def _apply_binary_quantization(self, embeddings: np.ndarray) -> np.ndarray:
        """Applique la binary quantization aux embeddings"""
        if not self.binary_quantization:
            return embeddings
        
        # Binary quantization: convertir en -1 ou +1
        quantized = np.where(embeddings > 0, 1.0, -1.0).astype(np.float32)
        
        # Normaliser pour maintenir la magnitude
        magnitude = np.linalg.norm(embeddings, axis=-1, keepdims=True)
        quantized = quantized * (magnitude / np.sqrt(self.dimension))
        
        return quantized
    
    def embed_documents(self, documents: List[Document]) -> List[List[float]]:
        """
        G√©n√®re les embeddings pour une liste de documents (images)
        
        Args:
            documents: Liste de documents LangChain contenant des images
            
        Returns:
            Liste des embeddings (vecteurs)
        """
        if not self.model or not self.processor:
            raise RuntimeError("Mod√®le MCDSE non initialis√©")
        
        logger.info(f"üñºÔ∏è  G√©n√©ration des embeddings pour {len(documents)} documents")
        
        # Extraire les images des documents
        images = []
        valid_docs = []
        
        for doc in documents:
            try:
                image_data = doc.metadata.get('image_data')
                if not image_data:
                    logger.warning(f"Pas de donn√©es d'image pour le document {doc.metadata.get('source_file')}")
                    continue
                
                # Convertir les donn√©es en image PIL
                image = Image.open(io.BytesIO(image_data))
                images.append(image)
                valid_docs.append(doc)
                
            except Exception as e:
                logger.error(f"Erreur lors du traitement de l'image: {e}")
                continue
        
        if not images:
            logger.warning("Aucune image valide trouv√©e")
            return []
        
        try:
            # Pr√©parer les inputs pour le mod√®le
            inputs = self.processor(
                text=[self.document_prompt] * len(images),
                images=[self._resize_image(img) for img in images],
                videos=None,
                padding='longest',
                return_tensors='pt'
            )
            
            # Pr√©parer les inputs pour la g√©n√©ration
            cache_position = torch.arange(0, len(images))
            inputs = self.model.prepare_inputs_for_generation(
                **inputs, cache_position=cache_position, use_cache=False
            )
            
            # G√©n√©rer les embeddings
            with torch.no_grad():
                outputs = self.model(
                    **inputs,
                    return_dict=True,
                    output_hidden_states=True
                )
                
                # Extraire les embeddings de la derni√®re couche cach√©e
                embeddings = outputs.hidden_states[-1][:, -1]
                
                # Normaliser et tronquer √† la dimension souhait√©e
                embeddings = torch.nn.functional.normalize(
                    embeddings[:, :self.dimension], p=2, dim=-1
                )
                
                # Convertir en numpy
                embeddings_np = embeddings.cpu().numpy()
                
                # Appliquer la binary quantization si activ√©e
                if self.binary_quantization:
                    embeddings_np = self._apply_binary_quantization(embeddings_np)
                
                logger.info(f"‚úÖ Embeddings g√©n√©r√©s: {embeddings_np.shape}")
                return embeddings_np.tolist()
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration des embeddings: {e}")
            raise
    
    def embed_query(self, query: str) -> List[float]:
        """
        G√©n√®re l'embedding pour une requ√™te textuelle
        
        Args:
            query: Texte de la requ√™te
            
        Returns:
            Vecteur d'embedding
        """
        if not self.model or not self.processor:
            raise RuntimeError("Mod√®le MCDSE non initialis√©")
        
        try:
            logger.info(f"üîç G√©n√©ration d'embedding pour la requ√™te: {query[:50]}...")
            
            # Cr√©er une image dummy pour les requ√™tes textuelles
            dummy_image = Image.new('RGB', (56, 56))
            
            # Pr√©parer les inputs
            inputs = self.processor(
                text=[self.query_prompt % query],
                images=[dummy_image],
                videos=None,
                padding='longest',
                return_tensors='pt'
            )
            
            # Pr√©parer pour la g√©n√©ration
            cache_position = torch.arange(0, 1)
            inputs = self.model.prepare_inputs_for_generation(
                **inputs, cache_position=cache_position, use_cache=False
            )
            
            # G√©n√©rer l'embedding
            with torch.no_grad():
                outputs = self.model(
                    **inputs,
                    return_dict=True,
                    output_hidden_states=True
                )
                
                # Extraire l'embedding
                embedding = outputs.hidden_states[-1][:, -1]
                
                # Normaliser et tronquer
                embedding = torch.nn.functional.normalize(
                    embedding[:, :self.dimension], p=2, dim=-1
                )
                
                # Convertir en numpy
                embedding_np = embedding.cpu().numpy()
                
                # Appliquer la binary quantization si activ√©e
                if self.binary_quantization:
                    embedding_np = self._apply_binary_quantization(embedding_np)
                
                return embedding_np[0].tolist()
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de l'embedding de requ√™te: {e}")
            # Fallback : retourner un vecteur z√©ro
            return [0.0] * self.dimension
