# =============================================================================
# RAG Newsletter - Service d'Embeddings MLX
# =============================================================================
# Service d'embeddings optimis√© pour Apple Silicon utilisant le mod√®le MCDSE-2B-V1
# avec MLX pour des performances maximales sur processeurs M4.
# =============================================================================

import io
import math
from typing import List

import torch
from langchain.schema import Document
from loguru import logger
from PIL import Image
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration


class MLXEmbeddingService:
    """
    Service d'embeddings optimis√© pour Apple Silicon M4 utilisant MLX.

    Ce service utilise le mod√®le MCDSE-2B-V1 (Multi-modal Cross-Document Search Embeddings)
    pour g√©n√©rer des embeddings de haute qualit√© √† partir d'images de documents PDF.
    Optimis√© pour les processeurs Apple Silicon avec Metal Performance Shaders (MPS).

    Fonctionnalit√©s cl√©s:
    - Embeddings d'images de documents (PDF ‚Üí image ‚Üí embedding)
    - Embeddings de requ√™tes textuelles
    - Optimisations Apple Silicon (MPS, MLX)
    - Normalisation L2 automatique
    - Gestion intelligente des contraintes de pixels du mod√®le

    Exemple d'utilisation:
        >>> service = MLXEmbeddingService()
        >>> embeddings = service.embed_documents(documents)
        >>> query_embedding = service.embed_query("sustainability strategy")
    """

    def __init__(self, model_name: str = "marco/mcdse-2b-v1", dimension: int = 1536):
        """
        Initialise le service d'embeddings MLX.

        Args:
            model_name (str): Nom du mod√®le HuggingFace √† utiliser.
                             Par d√©faut: "marco/mcdse-2b-v1" (optimis√© pour documents)
            dimension (int): Dimension des embeddings de sortie.
                           Par d√©faut: 1536 (dimension standard MCDSE)

        Raises:
            RuntimeError: Si le mod√®le ne peut pas √™tre charg√© ou si MLX n'est pas disponible
        """
        self.model_name = model_name
        self.dimension = dimension
        self.model = None
        self.processor = None

        # Initialisation du mod√®le (peut prendre plusieurs secondes)
        self._initialize_model()

        # Prompts optimis√©s pour le mod√®le MCDSE-2B-V1
        # Format sp√©cifique requis par le mod√®le pour l'encodage d'images
        self.document_prompt = (
            "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
            "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
            "What is shown in this image?<|im_end|>\n<|endoftext|>"
        )
        self.query_prompt = (
            "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
            "<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>"
            "Query: %s<|im_end|>\n<|endoftext|>"
        )

    def _initialize_model(self):
        """
        Initialise le mod√®le MCDSE-2B-V1 avec optimisations Apple Silicon.

        Cette m√©thode charge le mod√®le et le processeur avec des param√®tres optimis√©s
        pour les processeurs Apple Silicon M4, en utilisant Metal Performance Shaders (MPS)
        pour acc√©l√©rer les calculs sur GPU.

        Raises:
            RuntimeError: Si le mod√®le ne peut pas √™tre charg√© ou si les d√©pendances sont manquantes
        """
        try:
            logger.info(f"Chargement du mod√®le MCDSE: {self.model_name}")
            logger.info("‚è≥ Configuration pour Apple Silicon M4...")

            # Configuration des contraintes de pixels pour le mod√®le MCDSE-2B-V1
            # Le mod√®le a des limites strictes sur la taille des images d'entr√©e
            min_pixels = 1 * 28 * 28  # Minimum: 28x28 pixels
            max_pixels = 960 * 28 * 28  # Maximum: 960x28 pixels (contrainte du mod√®le)

            # Charger le processeur (tokenizer + image processor)
            logger.info("üì• Chargement du processeur...")
            self.processor = AutoProcessor.from_pretrained(
                self.model_name, min_pixels=min_pixels, max_pixels=max_pixels
            )

            # Charger le mod√®le principal avec optimisations Apple Silicon
            logger.info("üì• Chargement du mod√®le principal...")
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.model_name,
                torch_dtype=torch.bfloat16,  # Pr√©cision optimis√©e pour M4
                low_cpu_mem_usage=True,  # √âconomie de m√©moire RAM
            ).eval()  # Mode √©valuation (pas d'entra√Ænement)

            # D√©placer le mod√®le sur Metal Performance Shaders (MPS) si disponible
            if torch.backends.mps.is_available():
                logger.info("üçé D√©placement du mod√®le vers Apple Silicon MPS...")
                self.model = self.model.to("mps")
            else:
                logger.warning("‚ö†Ô∏è MPS non disponible, utilisation du CPU")

            # Configuration du padding pour la g√©n√©ration de texte
            # Padding √† gauche pour les mod√®les de g√©n√©ration
            self.model.padding_side = "left"
            self.processor.tokenizer.padding_side = "left"

            logger.info("‚úÖ Mod√®le MCDSE charg√© avec succ√®s sur Apple Silicon!")

        except Exception as e:
            logger.error(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
            raise RuntimeError(f"Impossible de charger le mod√®le MCDSE: {e}")

    def _smart_resize(
        self,
        height: int,
        width: int,
        max_pixels: int = 960 * 28 * 28,
        min_pixels: int = 1 * 28 * 28,
    ) -> tuple[int, int]:
        """
        Redimensionne intelligemment une image selon les contraintes du mod√®le MCDSE-2B-V1.

        Le mod√®le MCDSE-2B-V1 a des contraintes strictes sur la taille des images:
        - Les dimensions doivent √™tre des multiples de 28
        - Le nombre total de pixels doit √™tre entre min_pixels et max_pixels
        - L'aspect ratio doit √™tre pr√©serv√© autant que possible

        Args:
            height (int): Hauteur originale de l'image
            width (int): Largeur originale de l'image
            max_pixels (int): Nombre maximum de pixels autoris√©s (d√©faut: 960*28*28)
            min_pixels (int): Nombre minimum de pixels autoris√©s (d√©faut: 1*28*28)

        Returns:
            tuple[int, int]: Nouvelle hauteur et largeur respectant les contraintes

        Exemple:
            >>> service = MLXEmbeddingService()
            >>> new_h, new_w = service._smart_resize(1000, 800)
            >>> print(f"Nouvelles dimensions: {new_h}x{new_w}")
        """

        def round_by_factor(number: float, factor: int) -> int:
            """Arrondit un nombre au multiple le plus proche du facteur."""
            return round(number / factor) * factor

        def ceil_by_factor(number: float, factor: int) -> int:
            """Arrondit un nombre au multiple sup√©rieur du facteur."""
            return math.ceil(number / factor) * factor

        def floor_by_factor(number: float, factor: int) -> int:
            """Arrondit un nombre au multiple inf√©rieur du facteur."""
            return math.floor(number / factor) * factor

        # √âtape 1: Arrondir aux multiples de 28 (contrainte du mod√®le)
        h_bar = max(28, round_by_factor(height, 28))
        w_bar = max(28, round_by_factor(width, 28))

        # √âtape 2: V√©rifier si l'image est trop grande
        if h_bar * w_bar > max_pixels:
            # Calculer le facteur de r√©duction pour respecter max_pixels
            beta = math.sqrt((height * width) / max_pixels)
            h_bar = floor_by_factor(height / beta, 28)
            w_bar = floor_by_factor(width / beta, 28)
        # √âtape 3: V√©rifier si l'image est trop petite
        elif h_bar * w_bar < min_pixels:
            # Calculer le facteur d'agrandissement pour respecter min_pixels
            beta = math.sqrt(min_pixels / (height * width))
            h_bar = ceil_by_factor(height * beta, 28)
            w_bar = ceil_by_factor(width * beta, 28)

        return h_bar, w_bar

    def _resize_image(self, image: Image.Image) -> Image.Image:
        """
        Redimensionne une image selon les contraintes du mod√®le MCDSE-2B-V1.

        Args:
            image (Image.Image): Image PIL √† redimensionner

        Returns:
            Image.Image: Image redimensionn√©e respectant les contraintes du mod√®le
        """
        new_size = self._smart_resize(image.height, image.width)
        return image.resize(new_size, Image.Resampling.LANCZOS)

    def embed_documents(self, documents: List[Document]) -> List[List[float]]:
        """
        G√©n√®re les embeddings pour une liste de documents contenant des images.

        Cette m√©thode traite une liste de documents LangChain, extrait les images
        depuis leurs m√©tadonn√©es, et g√©n√®re des embeddings vectoriels optimis√©s
        pour la recherche s√©mantique.

        Args:
            documents (List[Document]): Liste de documents LangChain contenant des images.
                                      Chaque document doit avoir 'image_data' dans ses m√©tadonn√©es.

        Returns:
            List[List[float]]: Liste des embeddings vectoriels (1536 dimensions chacun).
                              Chaque embedding est normalis√© L2.

        Raises:
            RuntimeError: Si le mod√®le MCDSE n'est pas initialis√©
            ValueError: Si aucun document valide n'est fourni

        Exemple:
            >>> service = MLXEmbeddingService()
            >>> documents = [doc1, doc2, doc3]  # Documents avec image_data
            >>> embeddings = service.embed_documents(documents)
            >>> print(f"G√©n√©r√© {len(embeddings)} embeddings de {len(embeddings[0])} dimensions")
        """
        if not self.model or not self.processor:
            raise RuntimeError(
                "Mod√®le MCDSE non initialis√©. Appelez d'abord __init__()"
            )

        if not documents:
            raise ValueError("Aucun document fourni")

        embeddings = []
        successful_embeddings = 0

        logger.info(f"üñºÔ∏è  G√©n√©ration des embeddings pour {len(documents)} documents...")

        for i, doc in enumerate(documents):
            try:
                # R√©cup√©rer les donn√©es de l'image depuis les m√©tadonn√©es
                image_data = doc.metadata.get("image_data")
                if not image_data:
                    logger.warning(
                        f"Pas de donn√©es d'image pour le document {doc.metadata.get('source_file', 'inconnu')}"
                    )
                    continue

                # Convertir les donn√©es en image PIL
                try:
                    if isinstance(image_data, bytes):
                        # Donn√©es d'image en bytes (format le plus courant)
                        image = Image.open(io.BytesIO(image_data))
                    else:
                        # Chemin vers un fichier image
                        image = Image.open(image_data)

                    # V√©rifier que l'image est valide
                    if image.size[0] == 0 or image.size[1] == 0:
                        logger.warning(
                            f"Image invalide pour le document {doc.metadata.get('source_file', 'inconnu')}"
                        )
                        continue

                except Exception as img_error:
                    logger.warning(
                        f"Erreur lors du traitement de l'image pour {doc.metadata.get('source_file', 'inconnu')}: {img_error}"
                    )
                    continue

                # G√©n√©rer l'embedding pour l'image
                embedding = self._encode_document(image)
                embeddings.append(embedding)
                successful_embeddings += 1

                # Afficher la progression tous les 10 documents
                if (i + 1) % 10 == 0:
                    logger.info(
                        f"üìä Progression: {i + 1}/{len(documents)} documents trait√©s"
                    )

            except Exception as e:
                logger.error(f"Erreur lors de l'embedding du document {i}: {e}")
                continue

        logger.info(
            f"‚úÖ Embeddings g√©n√©r√©s avec succ√®s: {successful_embeddings}/{len(documents)} vecteurs"
        )
        return embeddings

    def _encode_document(self, image: Image.Image) -> List[float]:
        """
        Encode un document (image) en embedding vectoriel.

        Cette m√©thode utilise le mod√®le MCDSE-2B-V1 pour convertir une image
        de document en vecteur d'embedding de 1536 dimensions, optimis√© pour
        la recherche s√©mantique.

        Args:
            image (Image.Image): Image PIL du document √† encoder

        Returns:
            List[float]: Vecteur d'embedding normalis√© L2 de 1536 dimensions

        Raises:
            RuntimeError: Si l'encodage √©choue ou si le mod√®le n'est pas disponible

        Exemple:
            >>> service = MLXEmbeddingService()
            >>> image = Image.open("document.png")
            >>> embedding = service._encode_document(image)
            >>> print(f"Embedding g√©n√©r√©: {len(embedding)} dimensions")
        """
        try:
            # Pr√©parer les inputs pour le mod√®le MCDSE-2B-V1
            # Le mod√®le attend un prompt textuel + une image
            inputs = self.processor(
                text=[self.document_prompt],  # Prompt pour l'encodage d'image
                images=[
                    self._resize_image(image)
                ],  # Image redimensionn√©e selon les contraintes
                videos=None,  # Pas de vid√©o pour ce mod√®le
                padding="longest",  # Padding pour g√©rer les tailles variables
                return_tensors="pt",  # Retourner des tenseurs PyTorch
            )

            # D√©placer les inputs sur MPS (Metal Performance Shaders) pour Apple Silicon
            if torch.backends.mps.is_available():
                inputs = {k: v.to("mps") for k, v in inputs.items()}

            # Pr√©parer les inputs pour la g√©n√©ration (format requis par le mod√®le)
            cache_position = torch.arange(0, 1)  # Position du cache pour la g√©n√©ration
            inputs = self.model.prepare_inputs_for_generation(
                **inputs,
                cache_position=cache_position,
                use_cache=False,  # Pas de cache pour l'embedding
            )

            # G√©n√©rer l'embedding avec le mod√®le
            with torch.no_grad():  # Pas de calcul de gradient (mode inf√©rence)
                output = self.model(
                    **inputs,
                    return_dict=True,  # Retourner un dictionnaire
                    output_hidden_states=True,  # Inclure les √©tats cach√©s pour l'embedding
                )

                # Extraire l'embedding du dernier √©tat cach√©
                # [:, -1] = dernier token de la s√©quence
                embeddings = output.hidden_states[-1][:, -1]

                # Normaliser L2 et tronquer √† la dimension souhait√©e
                # La normalisation L2 am√©liore la qualit√© de la recherche s√©mantique
                normalized_embedding = torch.nn.functional.normalize(
                    embeddings[:, : self.dimension],  # Tronquer √† la dimension cible
                    p=2,  # Norme L2
                    dim=-1,  # Normaliser sur la derni√®re dimension
                )

                # Convertir en liste Python et retourner
                return normalized_embedding.cpu().squeeze().tolist()

        except Exception as e:
            logger.error(f"Erreur lors de l'encodage du document: {e}")
            raise RuntimeError(f"Impossible d'encoder le document: {e}")

    def embed_query(self, query: str) -> List[float]:
        """
        G√©n√®re l'embedding pour une requ√™te textuelle.

        Cette m√©thode convertit une requ√™te textuelle en vecteur d'embedding
        compatible avec les embeddings de documents, permettant la recherche
        s√©mantique dans la base vectorielle.

        Args:
            query (str): Texte de la requ√™te √† encoder (ex: "sustainability strategy")

        Returns:
            List[float]: Vecteur d'embedding normalis√© L2 de 1536 dimensions

        Raises:
            RuntimeError: Si le mod√®le MCDSE n'est pas initialis√©

        Exemple:
            >>> service = MLXEmbeddingService()
            >>> query = "What is TotalEnergies' sustainability strategy?"
            >>> embedding = service.embed_query(query)
            >>> print(f"Embedding de requ√™te: {len(embedding)} dimensions")
        """
        try:
            logger.info(f"üîç G√©n√©ration d'embedding pour la requ√™te: {query[:50]}...")

            # Cr√©er une image factice pour les requ√™tes textuelles
            # Le mod√®le MCDSE-2B-V1 n√©cessite toujours une image, m√™me pour du texte
            dummy_image = Image.new("RGB", (56, 56))  # Image noire 56x56 pixels

            # Pr√©parer les inputs avec le prompt de requ√™te
            inputs = self.processor(
                text=[self.query_prompt % query],  # Formatage du prompt avec la requ√™te
                images=[dummy_image],  # Image factice
                videos=None,  # Pas de vid√©o
                padding="longest",  # Padding pour g√©rer les tailles variables
                return_tensors="pt",  # Retourner des tenseurs PyTorch
            )

            # D√©placer sur MPS (Metal Performance Shaders) pour Apple Silicon
            if torch.backends.mps.is_available():
                inputs = {k: v.to("mps") for k, v in inputs.items()}

            # Pr√©parer pour la g√©n√©ration
            cache_position = torch.arange(0, 1)
            inputs = self.model.prepare_inputs_for_generation(
                **inputs, cache_position=cache_position, use_cache=False
            )

            # G√©n√©rer l'embedding
            with torch.no_grad():  # Mode inf√©rence (pas de gradient)
                output = self.model(
                    **inputs,
                    return_dict=True,  # Retourner un dictionnaire
                    output_hidden_states=True,  # Inclure les √©tats cach√©s
                )

                # Extraire et normaliser l'embedding
                embeddings = output.hidden_states[-1][:, -1]
                normalized_embedding = torch.nn.functional.normalize(
                    embeddings[:, : self.dimension],  # Tronquer √† la dimension cible
                    p=2,  # Norme L2
                    dim=-1,  # Normaliser sur la derni√®re dimension
                )

                return normalized_embedding.cpu().squeeze().tolist()

        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de l'embedding de requ√™te: {e}")
            # Fallback : retourner un vecteur z√©ro en cas d'erreur
            # Cela permet au syst√®me de continuer √† fonctionner m√™me en cas de probl√®me
            return [0.0] * self.dimension


# =============================================================================
# Alias de compatibilit√©
# =============================================================================
# Alias pour maintenir la compatibilit√© avec l'ancienne API
# Permet d'utiliser 'EmbeddingService' au lieu de 'MLXEmbeddingService'
# =============================================================================
EmbeddingService = MLXEmbeddingService
